from __future__ import annotations

import os
import re
import sys
from pathlib import Path
from typing import TypedDict, List

# Ensure project root is on sys.path when running as `python src/agent.py`
PROJECT_ROOT = Path(__file__).resolve().parents[1]
if str(PROJECT_ROOT) not in sys.path:
    sys.path.insert(0, str(PROJECT_ROOT))

from dotenv import load_dotenv
from langchain_openai import ChatOpenAI
from langchain_core.messages import BaseMessage, HumanMessage, AIMessage
from langgraph.graph import StateGraph, START, END
from langgraph.checkpoint.memory import MemorySaver

from src.rag import RAGSystem
from src.tools import get_current_weather
from src import memory as mem


load_dotenv()


class AgentState(TypedDict):
    messages: List[BaseMessage]
    context: str
    next_action: str
    session_id: str


llm = ChatOpenAI(model=os.getenv("OPENAI_MODEL", "gpt-4o-mini"), temperature=0)
rag_system: RAGSystem | None = None


def _ensure_rag() -> RAGSystem:
    global rag_system
    if rag_system is None:
        rag_system = RAGSystem()
    return rag_system


# 1) classify_query
def classify_query(state: AgentState) -> AgentState:
    user_msg = next((m.content for m in reversed(state["messages"]) if isinstance(m, HumanMessage)), "")
    prompt = (
        "Kullan覺c覺 sorusu d繹k羹man bilgisi mi gerektiriyor yoksa canl覺 hava durumu API'si mi?\n"
        "Sadece u yan覺tlardan birini ver: rag, weather, both.\n\n"
        f"Soru: {user_msg}"
    )
    res = llm.invoke(prompt)
    label = (res.content or "rag").strip().lower()
    if label not in {"rag", "weather", "both"}:
        label = "rag"
    state["next_action"] = label
    return state


# 2) rag_node
def rag_node(state: AgentState) -> AgentState:
    user_msg = next((m.content for m in reversed(state["messages"]) if isinstance(m, HumanMessage)), "")
    rag = _ensure_rag()
    context = rag.get_context_for_query(user_msg, k=3, max_chars=2000)
    state["context"] = context
    return state


# 3) weather_node
def _extract_city(question: str) -> str:
    # Basit LLM destekli 癟覺kar覺m + regex fallback
    sys_prompt = (
        "Aa覺daki c羹mlede ge癟en ehir ad覺n覺 tek kelime olarak d繹nd羹r.\n"
        "Sadece ehir ad覺n覺 yaz, baka bir ey yazma.\n"
        f"Metin: {question}"
    )
    try:
        out = llm.invoke(sys_prompt).content.strip()
        if out:
            return out.split("\n")[0].strip()
    except Exception:
        pass

    m = re.search(r"in |de |da |'da |'de |\bfor\b|\bof\b|\bin\b\s+([A-Z襤I][a-z癟覺羹繹覺]+)", question)
    if m:
        return m.group(1)
    return question.strip()


def weather_node(state: AgentState) -> AgentState:
    user_msg = next((m.content for m in reversed(state["messages"]) if isinstance(m, HumanMessage)), "")
    city = _extract_city(user_msg)
    weather_text = get_current_weather.invoke(city)
    # Context'e ekle
    prev = state.get("context", "")
    state["context"] = (prev + "\n\n" if prev else "") + weather_text
    return state


# 4) respond_node
def respond_node(state: AgentState) -> AgentState:
    session_id = state.get("session_id") or os.getenv("DEFAULT_SESSION_ID", "local-dev")
    history_items = mem.get_conversation_history(session_id, limit=10)
    history_text = "\n".join([f"{h['role']}: {h['content']}" for h in history_items])

    user_msg = next((m.content for m in reversed(state["messages"]) if isinstance(m, HumanMessage)), "")
    context = state.get("context", "")

    prompt = (
        "Aa覺daki balam覺 ve 繹nceki konuma ge癟miini kullanarak kullan覺c覺ya k覺sa, net ve T羹rk癟e cevap ver.\n"
        "Gerektiinde madde iaretleri ve emoji kullan.\n\n"
        f"[Ge癟mi]\n{history_text}\n\n[Balam]\n{context}\n\n[Soru]\n{user_msg}"
    )

    answer = llm.invoke(prompt).content

    # Memory kaydet
    mem.save_message(session_id, "user", user_msg)
    mem.save_message(session_id, "assistant", answer)

    state["messages"].append(AIMessage(content=answer))
    return state


def create_agent():
    workflow = StateGraph(AgentState)
    workflow.set_entry_point("classify")
    workflow.add_node("classify", classify_query)
    workflow.add_node("rag", rag_node)
    workflow.add_node("weather", weather_node)
    workflow.add_node("respond", respond_node)

    workflow.add_conditional_edges(
        "classify",
        lambda s: s["next_action"],
        {"rag": "rag", "weather": "weather", "both": "rag"},
    )

    workflow.add_edge("rag", "respond")
    workflow.add_edge("weather", "respond")
    workflow.add_edge("respond", END)

    memory = MemorySaver()
    app = workflow.compile(checkpointer=memory)
    return app


if __name__ == "__main__":
    from uuid import uuid4

    app = create_agent()
    sid = os.getenv("DEFAULT_SESSION_ID", str(uuid4()))

    def run(q: str):
        state = {
            "messages": [HumanMessage(content=q)],
            "context": "",
            "next_action": "rag",
            "session_id": sid,
        }
        out = app.invoke(state, config={"configurable": {"thread_id": sid}})
        last = out["messages"][-1]
        print(f"\nYou: {q}\n: {last.content}\n")

    # Tests
    run("API key nas覺l al覺n覺r?")
    run("Istanbul'da hava nas覺l?")
    run("API kullanarak Istanbul'un havas覺n覺 nas覺l 繹renebilirim?")
    run("Paris ve London'覺n s覺cakl覺klar覺n覺 kar覺lat覺r")
    run("Daha 繹nce hangi ehrin havas覺n覺 sormutum?")


